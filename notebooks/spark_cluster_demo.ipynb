{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Integration Demo\n",
    "\n",
    "This notebook demonstrates how to connect to the Spark cluster and work with:\n",
    "- Spark Master\n",
    "- MinIO (S3-compatible storage)\n",
    "- Hive Metastore\n",
    "\n",
    "## Environment Overview\n",
    "- **Spark Master**: spark://spark-master:7077\n",
    "- **MinIO**: http://minio:9000\n",
    "- **Hive Metastore**: thrift://hive-metastore:9083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Spark Session with Cluster Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T08:27:29.440553Z",
     "start_time": "2025-10-23T08:26:27.466389Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, sum as spark_sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import os\n",
    "\n",
    "# # Create Spark session connected to the cluster\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Jupyter Spark Demo\") \\\n",
    "#     .master(\"spark://spark-master:7077\") \\\n",
    "#     .config(\"spark.sql.warehouse.dir\", \"s3a://warehouse/\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "#     .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "#     .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "#     .config(\"spark.executor.memory\", \"1g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"1\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# print(f\"Spark Version: {spark.version}\")\n",
    "# print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "# print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/23 09:19:46 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+-------------+\n",
      "| id|   name| department|salary|     location|\n",
      "+---+-------+-----------+------+-------------+\n",
      "|  1|  Alice|Engineering| 95000|     New York|\n",
      "|  2|    Bob|      Sales| 75000|San Francisco|\n",
      "|  3|Charlie|Engineering|105000|     New York|\n",
      "|  4|  Diana|  Marketing| 80000|      Chicago|\n",
      "|  5|    Eve|Engineering| 98000|San Francisco|\n",
      "|  6|  Frank|      Sales| 72000|      Chicago|\n",
      "|  7|  Grace|  Marketing| 85000|     New York|\n",
      "|  8|  Henry|Engineering|110000|San Francisco|\n",
      "|  9|   Iris|      Sales| 78000|     New York|\n",
      "| 10|   Jack|  Marketing| 82000|      Chicago|\n",
      "+---+-------+-----------+------+-------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample employee data\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000, \"New York\"),\n",
    "    (2, \"Bob\", \"Sales\", 75000, \"San Francisco\"),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000, \"New York\"),\n",
    "    (4, \"Diana\", \"Marketing\", 80000, \"Chicago\"),\n",
    "    (5, \"Eve\", \"Engineering\", 98000, \"San Francisco\"),\n",
    "    (6, \"Frank\", \"Sales\", 72000, \"Chicago\"),\n",
    "    (7, \"Grace\", \"Marketing\", 85000, \"New York\"),\n",
    "    (8, \"Henry\", \"Engineering\", 110000, \"San Francisco\"),\n",
    "    (9, \"Iris\", \"Sales\", 78000, \"New York\"),\n",
    "    (10, \"Jack\", \"Marketing\", 82000, \"Chicago\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\", \"location\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter employees in Engineering department\n",
    "print(\"\\n=== Engineering Employees ===\")\n",
    "engineering_df = df.filter(col(\"department\") == \"Engineering\")\n",
    "engineering_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average salary by department\n",
    "print(\"\\n=== Average Salary by Department ===\")\n",
    "avg_salary_df = df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\")\n",
    ").orderBy(col(\"avg_salary\").desc())\n",
    "avg_salary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee count by location\n",
    "print(\"\\n=== Employees by Location ===\")\n",
    "location_df = df.groupBy(\"location\").agg(\n",
    "    count(\"*\").alias(\"employee_count\")\n",
    ").orderBy(col(\"employee_count\").desc())\n",
    "location_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Data to MinIO (S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet to MinIO\n",
    "output_path = \"s3a://data/employees/parquet\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back from MinIO\n",
    "df_from_s3 = spark.read.parquet(output_path)\n",
    "print(f\"\\nRecords read from S3: {df_from_s3.count()}\")\n",
    "df_from_s3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and Query Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS company\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a managed Hive table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"company.employees\")\n",
    "print(\"Table 'company.employees' created successfully\")\n",
    "\n",
    "# Show tables in the database\n",
    "spark.sql(\"SHOW TABLES IN company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Hive table using SQL\n",
    "print(\"\\n=== Query: Top 5 Highest Paid Employees ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary, location\n",
    "    FROM company.employees\n",
    "    ORDER BY salary DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex SQL query with aggregation\n",
    "print(\"\\n=== Query: Department Statistics ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as num_employees,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        MIN(salary) as min_salary,\n",
    "        MAX(salary) as max_salary\n",
    "    FROM company.employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE EXTENDED company.employees\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create an External Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create external table pointing to data in MinIO\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS company.employees_external (\n",
    "        id LONG,\n",
    "        name STRING,\n",
    "        department STRING,\n",
    "        salary LONG,\n",
    "        location STRING\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 's3a://data/employees/parquet'\n",
    "\"\"\")\n",
    "\n",
    "print(\"External table created successfully\")\n",
    "spark.sql(\"SHOW TABLES IN company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the external table\n",
    "spark.sql(\"SELECT * FROM company.employees_external\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitor Spark Jobs\n",
    "\n",
    "You can monitor your Spark jobs at:\n",
    "- Spark Master UI: http://localhost:8080\n",
    "- Spark Application UI: http://localhost:4040 (when job is running)\n",
    "- MinIO Console: http://localhost:9001 (admin/admin123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Spark context information\n",
    "sc = spark.sparkContext\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Spark UI: {sc.uiWebUrl}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to drop tables and database\n",
    "# spark.sql(\"DROP TABLE IF EXISTS company.employees\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS company.employees_external\")\n",
    "# spark.sql(\"DROP DATABASE IF EXISTS company\")\n",
    "# print(\"Cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Connecting to the Spark cluster from Jupyter\n",
    "2. ✅ Creating and manipulating DataFrames\n",
    "3. ✅ Saving and reading data from MinIO (S3)\n",
    "4. ✅ Creating and querying Hive tables\n",
    "5. ✅ Working with both managed and external tables\n",
    "6. ✅ Running SQL queries on distributed data\n",
    "\n",
    "You can now use this setup to:\n",
    "- Develop and test PySpark applications\n",
    "- Analyze large datasets\n",
    "- Build data pipelines\n",
    "- Prototype ML models with MLlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
