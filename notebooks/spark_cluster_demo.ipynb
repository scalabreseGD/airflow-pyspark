{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Integration Demo\n",
    "\n",
    "This notebook demonstrates how to connect to the Spark cluster and work with:\n",
    "- Spark Master\n",
    "- MinIO (S3-compatible storage)\n",
    "- Hive Metastore\n",
    "\n",
    "## Environment Overview\n",
    "- **Spark Master**: spark://spark-master:7077\n",
    "- **MinIO**: http://minio:9000\n",
    "- **Hive Metastore**: thrift://hive-metastore:9083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Spark Session with Cluster Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T08:27:29.440553Z",
     "start_time": "2025-10-23T08:26:27.466389Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/23 09:09:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.3\n",
      "Spark Master: spark://spark-master:7077\n",
      "Application ID: app-20251023090956-0001\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, sum as spark_sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import os\n",
    "\n",
    "# Create Spark session connected to the cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter Spark Demo\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://warehouse/\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/23 09:10:01 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+-------------+\n",
      "| id|   name| department|salary|     location|\n",
      "+---+-------+-----------+------+-------------+\n",
      "|  1|  Alice|Engineering| 95000|     New York|\n",
      "|  2|    Bob|      Sales| 75000|San Francisco|\n",
      "|  3|Charlie|Engineering|105000|     New York|\n",
      "|  4|  Diana|  Marketing| 80000|      Chicago|\n",
      "|  5|    Eve|Engineering| 98000|San Francisco|\n",
      "|  6|  Frank|      Sales| 72000|      Chicago|\n",
      "|  7|  Grace|  Marketing| 85000|     New York|\n",
      "|  8|  Henry|Engineering|110000|San Francisco|\n",
      "|  9|   Iris|      Sales| 78000|     New York|\n",
      "| 10|   Jack|  Marketing| 82000|      Chicago|\n",
      "+---+-------+-----------+------+-------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample employee data\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000, \"New York\"),\n",
    "    (2, \"Bob\", \"Sales\", 75000, \"San Francisco\"),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000, \"New York\"),\n",
    "    (4, \"Diana\", \"Marketing\", 80000, \"Chicago\"),\n",
    "    (5, \"Eve\", \"Engineering\", 98000, \"San Francisco\"),\n",
    "    (6, \"Frank\", \"Sales\", 72000, \"Chicago\"),\n",
    "    (7, \"Grace\", \"Marketing\", 85000, \"New York\"),\n",
    "    (8, \"Henry\", \"Engineering\", 110000, \"San Francisco\"),\n",
    "    (9, \"Iris\", \"Sales\", 78000, \"New York\"),\n",
    "    (10, \"Jack\", \"Marketing\", 82000, \"Chicago\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\", \"location\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Engineering Employees ===\n",
      "+---+-------+-----------+------+-------------+\n",
      "| id|   name| department|salary|     location|\n",
      "+---+-------+-----------+------+-------------+\n",
      "|  1|  Alice|Engineering| 95000|     New York|\n",
      "|  3|Charlie|Engineering|105000|     New York|\n",
      "|  5|    Eve|Engineering| 98000|San Francisco|\n",
      "|  8|  Henry|Engineering|110000|San Francisco|\n",
      "+---+-------+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter employees in Engineering department\n",
    "print(\"\\n=== Engineering Employees ===\")\n",
    "engineering_df = df.filter(col(\"department\") == \"Engineering\")\n",
    "engineering_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average Salary by Department ===\n",
      "+-----------+--------------+-----------------+\n",
      "| department|employee_count|       avg_salary|\n",
      "+-----------+--------------+-----------------+\n",
      "|Engineering|             4|         102000.0|\n",
      "|  Marketing|             3|82333.33333333333|\n",
      "|      Sales|             3|          75000.0|\n",
      "+-----------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate average salary by department\n",
    "print(\"\\n=== Average Salary by Department ===\")\n",
    "avg_salary_df = df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\")\n",
    ").orderBy(col(\"avg_salary\").desc())\n",
    "avg_salary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Employees by Location ===\n",
      "+-------------+--------------+\n",
      "|     location|employee_count|\n",
      "+-------------+--------------+\n",
      "|     New York|             4|\n",
      "|San Francisco|             3|\n",
      "|      Chicago|             3|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Employee count by location\n",
    "print(\"\\n=== Employees by Location ===\")\n",
    "location_df = df.groupBy(\"location\").agg(\n",
    "    count(\"*\").alias(\"employee_count\")\n",
    ").orderBy(col(\"employee_count\").desc())\n",
    "location_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Data to MinIO (S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: s3a://data/employees/parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save as Parquet to MinIO\n",
    "output_path = \"s3a://data/employees/parquet\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Records read from S3: 10\n",
      "+---+-------+-----------+------+-------------+\n",
      "| id|   name| department|salary|     location|\n",
      "+---+-------+-----------+------+-------------+\n",
      "|  1|  Alice|Engineering| 95000|     New York|\n",
      "|  2|    Bob|      Sales| 75000|San Francisco|\n",
      "|  3|Charlie|Engineering|105000|     New York|\n",
      "|  4|  Diana|  Marketing| 80000|      Chicago|\n",
      "|  5|    Eve|Engineering| 98000|San Francisco|\n",
      "+---+-------+-----------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read back from MinIO\n",
    "df_from_s3 = spark.read.parquet(output_path)\n",
    "print(f\"\\nRecords read from S3: {df_from_s3.count()}\")\n",
    "df_from_s3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and Query Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|analytics|\n",
      "|  company|\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS company\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/23 09:10:32 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'company.employees' created successfully\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  company|employees|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a managed Hive table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"company.employees\")\n",
    "print(\"Table 'company.employees' created successfully\")\n",
    "\n",
    "# Show tables in the database\n",
    "spark.sql(\"SHOW TABLES IN company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: Top 5 Highest Paid Employees ===\n",
      "+-------+-----------+------+-------------+\n",
      "|   name| department|salary|     location|\n",
      "+-------+-----------+------+-------------+\n",
      "|  Henry|Engineering|110000|San Francisco|\n",
      "|Charlie|Engineering|105000|     New York|\n",
      "|    Eve|Engineering| 98000|San Francisco|\n",
      "|  Alice|Engineering| 95000|     New York|\n",
      "|  Grace|  Marketing| 85000|     New York|\n",
      "+-------+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the Hive table using SQL\n",
    "print(\"\\n=== Query: Top 5 Highest Paid Employees ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary, location\n",
    "    FROM company.employees\n",
    "    ORDER BY salary DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: Department Statistics ===\n",
      "+-----------+-------------+----------+----------+----------+\n",
      "| department|num_employees|avg_salary|min_salary|max_salary|\n",
      "+-----------+-------------+----------+----------+----------+\n",
      "|Engineering|            4|  102000.0|     95000|    110000|\n",
      "|  Marketing|            3|  82333.33|     80000|     85000|\n",
      "|      Sales|            3|   75000.0|     72000|     78000|\n",
      "+-----------+-------------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complex SQL query with aggregation\n",
    "print(\"\\n=== Query: Department Statistics ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as num_employees,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        MIN(salary) as min_salary,\n",
    "        MAX(salary) as max_salary\n",
    "    FROM company.employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                    |comment|\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|id                          |bigint                                                       |NULL   |\n",
      "|name                        |string                                                       |NULL   |\n",
      "|department                  |string                                                       |NULL   |\n",
      "|salary                      |bigint                                                       |NULL   |\n",
      "|location                    |string                                                       |NULL   |\n",
      "|                            |                                                             |       |\n",
      "|# Detailed Table Information|                                                             |       |\n",
      "|Catalog                     |spark_catalog                                                |       |\n",
      "|Database                    |company                                                      |       |\n",
      "|Table                       |employees                                                    |       |\n",
      "|Owner                       |jovyan                                                       |       |\n",
      "|Created Time                |Thu Oct 23 09:10:33 GMT 2025                                 |       |\n",
      "|Last Access                 |UNKNOWN                                                      |       |\n",
      "|Created By                  |Spark 3.5.3                                                  |       |\n",
      "|Type                        |MANAGED                                                      |       |\n",
      "|Provider                    |parquet                                                      |       |\n",
      "|Statistics                  |3384 bytes                                                   |       |\n",
      "|Location                    |s3a://warehouse/company.db/employees                         |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe  |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat|       |\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE EXTENDED company.employees\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create an External Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External table created successfully\n",
      "+---------+------------------+-----------+\n",
      "|namespace|         tableName|isTemporary|\n",
      "+---------+------------------+-----------+\n",
      "|  company|         employees|      false|\n",
      "|  company|employees_external|      false|\n",
      "+---------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create external table pointing to data in MinIO\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS company.employees_external (\n",
    "        id LONG,\n",
    "        name STRING,\n",
    "        department STRING,\n",
    "        salary LONG,\n",
    "        location STRING\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 's3a://data/employees/parquet'\n",
    "\"\"\")\n",
    "\n",
    "print(\"External table created successfully\")\n",
    "spark.sql(\"SHOW TABLES IN company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+-------------+\n",
      "| id|   name| department|salary|     location|\n",
      "+---+-------+-----------+------+-------------+\n",
      "|  1|  Alice|Engineering| 95000|     New York|\n",
      "|  2|    Bob|      Sales| 75000|San Francisco|\n",
      "|  3|Charlie|Engineering|105000|     New York|\n",
      "|  4|  Diana|  Marketing| 80000|      Chicago|\n",
      "|  5|    Eve|Engineering| 98000|San Francisco|\n",
      "|  6|  Frank|      Sales| 72000|      Chicago|\n",
      "|  7|  Grace|  Marketing| 85000|     New York|\n",
      "|  8|  Henry|Engineering|110000|San Francisco|\n",
      "|  9|   Iris|      Sales| 78000|     New York|\n",
      "| 10|   Jack|  Marketing| 82000|      Chicago|\n",
      "+---+-------+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the external table\n",
    "spark.sql(\"SELECT * FROM company.employees_external\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitor Spark Jobs\n",
    "\n",
    "You can monitor your Spark jobs at:\n",
    "- Spark Master UI: http://localhost:8080\n",
    "- Spark Application UI: http://localhost:4040 (when job is running)\n",
    "- MinIO Console: http://localhost:9001 (admin/admin123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application Name: Jupyter Spark Demo\n",
      "Application ID: app-20251023090956-0001\n",
      "Master: spark://spark-master:7077\n",
      "Spark UI: http://17e1683f71b9:4040\n",
      "Default Parallelism: 2\n"
     ]
    }
   ],
   "source": [
    "# Get Spark context information\n",
    "sc = spark.sparkContext\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Spark UI: {sc.uiWebUrl}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to drop tables and database\n",
    "# spark.sql(\"DROP TABLE IF EXISTS company.employees\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS company.employees_external\")\n",
    "# spark.sql(\"DROP DATABASE IF EXISTS company\")\n",
    "# print(\"Cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Connecting to the Spark cluster from Jupyter\n",
    "2. ✅ Creating and manipulating DataFrames\n",
    "3. ✅ Saving and reading data from MinIO (S3)\n",
    "4. ✅ Creating and querying Hive tables\n",
    "5. ✅ Working with both managed and external tables\n",
    "6. ✅ Running SQL queries on distributed data\n",
    "\n",
    "You can now use this setup to:\n",
    "- Develop and test PySpark applications\n",
    "- Analyze large datasets\n",
    "- Build data pipelines\n",
    "- Prototype ML models with MLlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
