{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Integration Demo\n",
    "\n",
    "This notebook demonstrates how to connect to the Spark cluster and work with:\n",
    "- Spark Master\n",
    "- MinIO (S3-compatible storage)\n",
    "- Hive Metastore\n",
    "\n",
    "## Environment Overview\n",
    "- **Spark Master**: spark://spark-master:7077\n",
    "- **MinIO**: http://minio:9000\n",
    "- **Hive Metastore**: thrift://hive-metastore:9083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Spark Session with Cluster Connection"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T14:31:30.556653Z",
     "start_time": "2025-10-23T14:31:30.403709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.sql('SHOW TABLES IN bronze').show(truncate=False)\n",
    "# spark.sql('SELECT * FROM bronze.campaign_events_raw').show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+-----------+\n",
      "|namespace|tableName                |isTemporary|\n",
      "+---------+-------------------------+-----------+\n",
      "|bronze   |campaign_events_raw      |false      |\n",
      "|bronze   |customer_interactions_raw|false      |\n",
      "|bronze   |inventory_snapshots_raw  |false      |\n",
      "|bronze   |marketing_campaigns_raw  |false      |\n",
      "|bronze   |product_catalog_raw      |false      |\n",
      "|bronze   |subscriptions_raw        |false      |\n",
      "|bronze   |transaction_items_raw    |false      |\n",
      "|bronze   |transactions_raw         |false      |\n",
      "+---------+-------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:19:30.039448Z",
     "start_time": "2025-10-23T15:19:29.832933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tables = [\n",
    "# 'campaign_events_raw',\n",
    "# 'customer_interactions_raw',\n",
    "# 'inventory_snapshots_raw',\n",
    "'marketing_campaigns_raw',\n",
    "'product_catalog_raw',\n",
    "# 'subscriptions_raw',\n",
    "# 'transaction_items_raw',\n",
    "# 'transactions_raw',\n",
    "]\n",
    "for table in tables:\n",
    "    spark.sql(f'SELECT * FROM bronze.{table}').show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------+----------+----------+----------+--------+--------------------+--------------------+--------------------+-------------------+--------------+--------------------+\n",
      "|campaign_id|       campaign_name|campaign_type|   channel|start_date|  end_date|  budget|     target_audience|         creative_id|      _source_system|         _file_name|_record_offset|_ingestion_timestamp|\n",
      "+-----------+--------------------+-------------+----------+----------+----------+--------+--------------------+--------------------+--------------------+-------------------+--------------+--------------------+\n",
      "|    CAMP001|      Fall Sale 2025|        email|     email|2025-10-01|2025-10-31|15000.00|\"{\"\"age_range\"\":\"...|\"\"interests\"\":[\"\"...|     \"\"home-decor\"\"]|            CRE1001|          NULL|                NULL|\n",
      "|    CAMP002|Black Friday Preview|       social|  facebook|2025-10-15|2025-11-15|25000.00|\"{\"\"age_range\"\":\"...|\"\"interests\"\":[\"\"...|          \"\"deals\"\"]|            CRE1002|          NULL|                NULL|\n",
      "|    CAMP003| Tech Week Promotion|      display|google_ads|2025-10-10|2025-10-24|18000.00|\"{\"\"age_range\"\":\"...|\"\"interests\"\":[\"\"...|        \"\"gadgets\"\"]|            CRE1003|          NULL|                NULL|\n",
      "|    CAMP004|     Fitness October|        email|     email|2025-10-01|2025-10-31| 8000.00|\"{\"\"age_range\"\":\"...|\"\"interests\"\":[\"\"...|         \"\"health\"\"]|            CRE1004|          NULL|                NULL|\n",
      "|    CAMP005|Instagram Stories...|       social| instagram|2025-10-18|2025-10-20| 5000.00|\"{\"\"age_range\"\":\"...|\"\"interests\"\":[\"\"...|      \"\"lifestyle\"\"]|            CRE1005|          NULL|                NULL|\n",
      "|    CAMP006|  Holiday Gift Guide|       search|google_ads|2025-10-20|2025-12-25|35000.00|\"{\"\"age_range\"\":\"...|\"\"interests\"\":[\"\"...|       \"\"shopping\"\"]|            CRE1006|          NULL|                NULL|\n",
      "|    CAMP007|VIP Customer Excl...|        email|     email|2025-10-15|2025-10-22| 3000.00|\"{\"\"age_range\"\":\"...|\"\"segment\"\":\"\"vip...|\"\"min_lifetime_va...| marketing_platform|          NULL|                NULL|\n",
      "|    CAMP008|YouTube Product R...|       social|   youtube|2025-10-01|2025-10-31|12000.00|\"{\"\"age_range\"\":\"...|\"\"interests\"\":[\"\"...|           \"\"tech\"\"]|            CRE1008|          NULL|                NULL|\n",
      "|    CAMP009|Retargeting Cart ...|      display|    criteo|2025-10-01|2025-10-31|10000.00|\"{\"\"segment\"\":\"\"c...|\"\"days_since_aban...|             CRE1009|2025-10-01 08:00:00|          NULL|                NULL|\n",
      "|    CAMP010|Affiliate Partner...|    affiliate|    impact|2025-10-01|2025-10-31|20000.00|\"{\"\"affiliate_tie...|\"\"commission_rate...|             CRE1010|2025-10-01 08:00:00|          NULL|                NULL|\n",
      "|    CAMP011|TikTok Trending C...|       social|    tiktok|2025-10-12|2025-10-26| 8500.00|\"{\"\"age_range\"\":\"...|\"\"interests\"\":[\"\"...|     \"\"challenges\"\"]|            CRE1011|          NULL|                NULL|\n",
      "|    CAMP012|Customer Winback ...|        email|     email|2025-10-05|2025-10-31| 6000.00|\"{\"\"segment\"\":\"\"c...|\"\"last_purchase_d...|             CRE1012|2025-10-05 08:00:00|          NULL|                NULL|\n",
      "|    CAMP013|   LinkedIn B2B Tech|       social|  linkedin|2025-10-01|2025-11-30|15000.00|\"{\"\"job_titles\"\":...|      \"\"IT-Manager\"\"|      \"\"Developer\"\"]|            CRE1013|          NULL|                NULL|\n",
      "|    CAMP014|Mobile App Instal...|      display|  facebook|2025-10-08|2025-10-31| 9000.00|\"{\"\"age_range\"\":\"...|\"\"device\"\":\"\"mobi...|\"\"segment\"\":\"\"non...| marketing_platform|          NULL|                NULL|\n",
      "|    CAMP015| Subscription Upsell|        email|     email|2025-10-10|2025-10-31| 4000.00|\"{\"\"segment\"\":\"\"b...|\"\"tenure_months\"\"...|             CRE1015|2025-10-10 08:00:00|          NULL|                NULL|\n",
      "+-----------+--------------------+-------------+----------+----------+----------+--------+--------------------+--------------------+--------------------+-------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T08:27:29.440553Z",
     "start_time": "2025-10-23T08:26:27.466389Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, sum as spark_sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import os\n",
    "\n",
    "# # Create Spark session connected to the cluster\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Jupyter Spark Demo\") \\\n",
    "#     .master(\"spark://spark-master:7077\") \\\n",
    "#     .config(\"spark.sql.warehouse.dir\", \"s3a://warehouse/\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "#     .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "#     .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "#     .config(\"spark.executor.memory\", \"1g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"1\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# print(f\"Spark Version: {spark.version}\")\n",
    "# print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "# print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/23 11:13:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+------+-------------+\n",
      "| id|   name| department|salary|     location|\n",
      "+---+-------+-----------+------+-------------+\n",
      "|  1|  Alice|Engineering| 95000|     New York|\n",
      "|  2|    Bob|      Sales| 75000|San Francisco|\n",
      "|  3|Charlie|Engineering|105000|     New York|\n",
      "|  4|  Diana|  Marketing| 80000|      Chicago|\n",
      "|  5|    Eve|Engineering| 98000|San Francisco|\n",
      "|  6|  Frank|      Sales| 72000|      Chicago|\n",
      "|  7|  Grace|  Marketing| 85000|     New York|\n",
      "|  8|  Henry|Engineering|110000|San Francisco|\n",
      "|  9|   Iris|      Sales| 78000|     New York|\n",
      "| 10|   Jack|  Marketing| 82000|      Chicago|\n",
      "+---+-------+-----------+------+-------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample employee data\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000, \"New York\"),\n",
    "    (2, \"Bob\", \"Sales\", 75000, \"San Francisco\"),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000, \"New York\"),\n",
    "    (4, \"Diana\", \"Marketing\", 80000, \"Chicago\"),\n",
    "    (5, \"Eve\", \"Engineering\", 98000, \"San Francisco\"),\n",
    "    (6, \"Frank\", \"Sales\", 72000, \"Chicago\"),\n",
    "    (7, \"Grace\", \"Marketing\", 85000, \"New York\"),\n",
    "    (8, \"Henry\", \"Engineering\", 110000, \"San Francisco\"),\n",
    "    (9, \"Iris\", \"Sales\", 78000, \"New York\"),\n",
    "    (10, \"Jack\", \"Marketing\", 82000, \"Chicago\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\", \"location\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter employees in Engineering department\n",
    "print(\"\\n=== Engineering Employees ===\")\n",
    "engineering_df = df.filter(col(\"department\") == \"Engineering\")\n",
    "engineering_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average salary by department\n",
    "print(\"\\n=== Average Salary by Department ===\")\n",
    "avg_salary_df = df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\")\n",
    ").orderBy(col(\"avg_salary\").desc())\n",
    "avg_salary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee count by location\n",
    "print(\"\\n=== Employees by Location ===\")\n",
    "location_df = df.groupBy(\"location\").agg(\n",
    "    count(\"*\").alias(\"employee_count\")\n",
    ").orderBy(col(\"employee_count\").desc())\n",
    "location_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Data to MinIO (S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet to MinIO\n",
    "output_path = \"s3a://data/employees/parquet\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back from MinIO\n",
    "df_from_s3 = spark.read.parquet(output_path)\n",
    "print(f\"\\nRecords read from S3: {df_from_s3.count()}\")\n",
    "df_from_s3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and Query Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS company\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a managed Hive table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"company.employees\")\n",
    "print(\"Table 'company.employees' created successfully\")\n",
    "\n",
    "# Show tables in the database\n",
    "spark.sql(\"SHOW TABLES IN company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Hive table using SQL\n",
    "print(\"\\n=== Query: Top 5 Highest Paid Employees ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary, location\n",
    "    FROM company.employees\n",
    "    ORDER BY salary DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex SQL query with aggregation\n",
    "print(\"\\n=== Query: Department Statistics ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as num_employees,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        MIN(salary) as min_salary,\n",
    "        MAX(salary) as max_salary\n",
    "    FROM company.employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE EXTENDED company.employees\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create an External Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create external table pointing to data in MinIO\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS company.employees_external (\n",
    "        id LONG,\n",
    "        name STRING,\n",
    "        department STRING,\n",
    "        salary LONG,\n",
    "        location STRING\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 's3a://data/employees/parquet'\n",
    "\"\"\")\n",
    "\n",
    "print(\"External table created successfully\")\n",
    "spark.sql(\"SHOW TABLES IN company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the external table\n",
    "spark.sql(\"SELECT * FROM company.employees_external\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitor Spark Jobs\n",
    "\n",
    "You can monitor your Spark jobs at:\n",
    "- Spark Master UI: http://localhost:8080\n",
    "- Spark Application UI: http://localhost:4040 (when job is running)\n",
    "- MinIO Console: http://localhost:9001 (admin/admin123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Spark context information\n",
    "sc = spark.sparkContext\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Spark UI: {sc.uiWebUrl}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to drop tables and database\n",
    "# spark.sql(\"DROP TABLE IF EXISTS company.employees\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS company.employees_external\")\n",
    "# spark.sql(\"DROP DATABASE IF EXISTS company\")\n",
    "# print(\"Cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Connecting to the Spark cluster from Jupyter\n",
    "2. ✅ Creating and manipulating DataFrames\n",
    "3. ✅ Saving and reading data from MinIO (S3)\n",
    "4. ✅ Creating and querying Hive tables\n",
    "5. ✅ Working with both managed and external tables\n",
    "6. ✅ Running SQL queries on distributed data\n",
    "\n",
    "You can now use this setup to:\n",
    "- Develop and test PySpark applications\n",
    "- Analyze large datasets\n",
    "- Build data pipelines\n",
    "- Prototype ML models with MLlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
